knitr::opts_chunk$set(echo = TRUE)
library(keras)
library(here)
library(httr)
library(jsonlite)
library(tidyverse)
# devtools::install_github("rstudio/tfdeploy")
library(tfdeploy)
model_dense <- load_model_hdf5("models/mnist_dense_hdf5.h5")
load_model_weights_hdf5(model_dense, "models/mnist_dense_weights_hdf5.h5")
mnist <- dataset_mnist()
train_images <- mnist$train$x
train_labels <- mnist$train$y
# TODO improve
# TODO show adding a layer from another model
export_savedmodel(model_dense, "models/savedmodel")
view_savedmodel("models/savedmodel")
serve_savedmodel("models/savedmodel", daemonized=TRUE)
# grab a random index from the "batch" index (the first axis)
digit_index <- sample.int(length(train_images[1,,]), 1)
digit <- train_images[digit_index,,] # <- one slice of tensor please :)
digit_raw <- base64enc::base64encode(digit)
plot(as.raster(digit, max=255))
knitr::opts_chunk$set(echo = TRUE)
library(keras)
library(here)
library(httr)
library(jsonlite)
library(tidyverse)
# devtools::install_github("rstudio/tfdeploy")
library(tfdeploy)
model_dense <- load_model_hdf5("models/mnist_dense_hdf5.h5")
library(here)
library(keras)
library(tidyverse)
mnist <- dataset_mnist()
str(mnist$train)
train_images <- mnist$train$x
train_labels <- mnist$train$y
paste("How many dimensions does this tensor have?")
length(dim(train_images))
paste("What shape does this tensor have?")
dim(train_images)
paste("What datatype does this tensor have?")
typeof(train_images)
# grab a random index from the "batch" index (the first axis)
digit_index <- sample.int(length(train_images[1,,]), 1)
digit <- train_images[digit_index,,] # <- one slice of tensor please :)
plot(as.raster(digit, max=255))
train_images <- array_reshape(train_images, c(60000, 28 * 28))
train_images <- train_images / 255
head(train_images)
train_labels <- to_categorical(train_labels)
model <- keras_model_sequential() %>% # initialize our model using a pre-baked model type (thanks R keras!)
layer_dense(units = 512, activation = "relu", input_shape = c(28 * 28)) %>% # define our first layer transformation
layer_dense(units = 10, activation = "softmax") # define the final layer transformation
basic_relu <- function(my_2d_tensor) {
for (row in nrow(my_2d_tensor)) {
for (column in ncol(my_2d_tensor)) {
# the individual entry
my_2d_tensor[row, column] <- max(my_2d_tensor[row, column], 0)
}
}
}
model %>% compile(
optimizer = "rmsprop",
loss = "categorical_crossentropy",
metrics = c("accuracy")
)
test_images <- mnist$test$x
test_images <- array_reshape(test_images, c(10000, 28 * 28))
test_images <- test_images / 255
test_labels <- mnist$test$y
test_labels <- to_categorical(test_labels)
model
get_weights(model)
library(here)
library(keras)
library(tidyverse)
mnist <- dataset_mnist()
str(mnist$train)
train_images <- mnist$train$x
train_labels <- mnist$train$y
paste("How many dimensions does this tensor have?")
length(dim(train_images))
paste("What shape does this tensor have?")
dim(train_images)
paste("What datatype does this tensor have?")
typeof(train_images)
# grab a random index from the "batch" index (the first axis)
digit_index <- sample.int(length(train_images[1,,]), 1)
digit <- train_images[digit_index,,] # <- one slice of tensor please :)
plot(as.raster(digit, max=255))
train_images <- array_reshape(train_images, c(60000, 28 * 28))
train_images <- train_images / 255
head(train_images)
train_labels <- to_categorical(train_labels)
model <- keras_model_sequential() %>% # initialize our model using a pre-baked model type (thanks R keras!)
layer_dense(units = 512, activation = "relu", input_shape = c(28 * 28)) %>% # define our first layer transformation
layer_dense(units = 10, activation = "softmax") # define the final layer transformation
basic_relu <- function(my_2d_tensor) {
for (row in nrow(my_2d_tensor)) {
for (column in ncol(my_2d_tensor)) {
# the individual entry
my_2d_tensor[row, column] <- max(my_2d_tensor[row, column], 0)
}
}
}
model %>% compile(
optimizer = "rmsprop",
loss = "categorical_crossentropy",
metrics = c("accuracy")
)
test_images <- mnist$test$x
test_images <- array_reshape(test_images, c(10000, 28 * 28))
test_images <- test_images / 255
test_labels <- mnist$test$y
test_labels <- to_categorical(test_labels)
model
get_weights(model)
# replace the "" with the name of one of the layers printed in the previous step
get_layer(model, name="dense_1")
# replace the "" with the name of one of the layers printed in the previous step
get_layer(model, name="dense_31")
# replace the "" with the name of one of the layers printed in the previous step
get_layer(model, name="dense_3")
get_weights(model)
history <- model %>% fit(train_images, train_labels, epochs = 5, batch_size = 128)
library(here)
library(keras)
library(tidyverse)
mnist <- dataset_mnist()
str(mnist$train)
train_images <- mnist$train$x
train_labels <- mnist$train$y
paste("How many dimensions does this tensor have?")
length(dim(train_images))
paste("What shape does this tensor have?")
dim(train_images)
paste("What datatype does this tensor have?")
typeof(train_images)
# grab a random index from the "batch" index (the first axis)
digit_index <- sample.int(length(train_images[1,,]), 1)
digit <- train_images[digit_index,,] # <- one slice of tensor please :)
plot(as.raster(digit, max=255))
train_images <- array_reshape(train_images, c(60000, 28 * 28))
train_images <- train_images / 255
head(train_images)
train_labels <- to_categorical(train_labels)
model <- keras_model_sequential() %>% # initialize our model using a pre-baked model type (thanks R keras!)
layer_dense(units = 512, activation = "relu", input_shape = c(28 * 28)) %>% # define our first layer transformation
layer_dense(units = 10, activation = "softmax") # define the final layer transformation
basic_relu <- function(my_2d_tensor) {
for (row in nrow(my_2d_tensor)) {
for (column in ncol(my_2d_tensor)) {
# the individual entry
my_2d_tensor[row, column] <- max(my_2d_tensor[row, column], 0)
}
}
}
model %>% compile(
optimizer = "rmsprop",
loss = "categorical_crossentropy",
metrics = c("accuracy")
)
test_images <- mnist$test$x
test_images <- array_reshape(test_images, c(10000, 28 * 28))
test_images <- test_images / 255
test_labels <- mnist$test$y
test_labels <- to_categorical(test_labels)
model
get_weights(model)
model %>% compile(
optimizer = "rmsprop",
loss = "categorical_crossentropy",
metrics = c("accuracy")
)
model
get_weights(model)
library(keras)
mnist <- dataset_mnist()
str(mnist$train)
train_images <- mnist$train$x
train_labels <- mnist$train$y
paste("How many dimensions does this tensor have?")
length(dim(train_images))
paste("What shape does this tensor have?")
dim(train_images)
paste("What datatype does this tensor have?")
typeof(train_images)
# grab a random index from the "batch" index (the first axis)
digit_index <- sample.int(length(train_images[1,,]), 1)
digit <- train_images[digit_index,,] # <- one slice of tensor please :)
plot(as.raster(digit, max=255))
train_images <- array_reshape(train_images, c(60000, 28 * 28))
train_images <- train_images / 255
train_images
train_labels <- to_categorical(train_labels)
model <- keras_model_sequential() %>% # initialize our model using a pre-baked model type (thanks R keras!)
layer_dense(units = 512, activation = "relu", input_shape = c(28 * 28)) %>% # define our first layer transformation
layer_dense(units = 10, activation = "softmax") # define the final layer transformation
basic_relu <- function(my_2d_tensor) {
for (row in nrow(my_2d_tensor)) {
for (column in ncol(my_2d_tensor)) {
# the individual entry
my_2d_tensor[row, column] <- max(my_2d_tensor[row, column], 0)
}
}
}
model %>% compile(
optimizer = "rmsprop",
loss = "categorical_crossentropy",
metrics = c("accuracy")
)
test_images <- mnist$test$x
test_images <- array_reshape(test_images, c(10000, 28 * 28))
test_images <- test_images / 255
test_labels <- mnist$test$y
test_labels <- to_categorical(test_labels)
model
get_weights(model)
library(here)
library(keras)
library(tidyverse)
mnist <- dataset_mnist()
str(mnist$train)
train_images <- mnist$train$x
train_labels <- mnist$train$y
paste("How many dimensions does this tensor have?")
length(dim(train_images))
paste("What shape does this tensor have?")
dim(train_images)
paste("What datatype does this tensor have?")
typeof(train_images)
# grab a random index from the "batch" index (the first axis)
digit_index <- sample.int(length(train_images[1,,]), 1)
digit <- train_images[digit_index,,] # <- one slice of tensor please :)
plot(as.raster(digit, max=255))
train_images <- array_reshape(train_images, c(60000, 28 * 28))
train_images <- train_images / 255
head(train_images)
train_labels <- to_categorical(train_labels)
model <- keras_model_sequential() %>% # initialize our model using a pre-baked model type (thanks R keras!)
layer_dense(units = 512, activation = "relu", input_shape = c(28 * 28)) %>% # define our first layer transformation
layer_dense(units = 10, activation = "softmax") # define the final layer transformation
basic_relu <- function(my_2d_tensor) {
for (row in nrow(my_2d_tensor)) {
for (column in ncol(my_2d_tensor)) {
# the individual entry
my_2d_tensor[row, column] <- max(my_2d_tensor[row, column], 0)
}
}
}
model %>% compile(
optimizer = "rmsprop",
loss = "categorical_crossentropy",
metrics = c("accuracy")
)
test_images <- mnist$test$x
test_images <- array_reshape(test_images, c(10000, 28 * 28))
test_images <- test_images / 255
test_labels <- mnist$test$y
test_labels <- to_categorical(test_labels)
model
get_weights(model)
history <- model %>% fit(train_images, train_labels, epochs = 5, batch_size = 128)
write(model_to_yaml(model), "models/mnist.yaml")
cmp_model <- model
write(serialize_model(cmp_model), "models/mnist_cmp.txt")
metrics <- model %>% evaluate(test_images, test_labels, verbose = 0)
metrics
# no lines on the plot for some reason
plot(history)
library(keras)
mnist <- dataset_mnist()
train_images <- mnist$train$x
train_labels <- mnist$train$y
test_images <- mnist$test$x
test_labels <- mnist$test$y
model <- keras_model_sequential() %>%
layer_dense(units = 512, activation = "relu", input_shape = c(28 * 28)) %>%
layer_dense(units = 10, activation = "softmax")
model %>% compile(
optimizer = "rmsprop",
loss = "categorical_crossentropy",
metrics = c("accuracy")
)
train_images <- array_reshape(train_images, c(60000, 28 * 28))
train_images <- train_images / 255
test_images <- array_reshape(test_images, c(10000, 28 * 28))
test_images <- test_images / 255
train_labels <- to_categorical(train_labels)
test_labels <- to_categorical(test_labels)
metrics <- model %>% evaluate(test_images, test_labels, verbose = 0)
history <- model %>% fit(train_images, train_labels, epochs = 5, batch_size = 128)
install.packages("lime")
# not sure why lines aren't showing up in the plot :/
plot(history)
metrics
preds <- predict(model, test_images)
preds_df <- as.data.frame(preds)
names(preds_df) <- c(0:9) # wasn't working in the data.frame cast for some reason
predictions <- preds_df %>%
mutate(digit_index = row_number()) %>%
gather(number, probability, 1:10)
predictions_summary <- predictions %>%
group_by(digit_index) %>%
summarize(likely_number = number[which.max(probability)])
predictions_summary_sample <- sample_n(predictions_summary, 24)
for (n in predictions_summary_sample$digit_index) {
pred_img <- mnist$test$x[n,,]
plot(as.raster(pred_img, max=255))
title(paste("Predicted number for index", n, ":", predictions_summary$likely_number[n]))
}
raw_model <- model
write(serialize_model(raw_model), "models/mnist_raw.txt")
write(model_to_yaml(model), "models/mnist_dense.yaml")
install.packages("cloudml")
knitr::opts_chunk$set(echo = TRUE)
library(keras)
library(here)
library(httr)
library(jsonlite)
library(tidyverse)
# devtools::install_github("rstudio/tfdeploy")
library(tfdeploy)
model_dense <- load_model_hdf5("models/mnist_dense_hdf5.h5")
load_model_weights_hdf5(model_dense, "models/mnist_dense_weights_hdf5.h5")
mnist <- dataset_mnist()
train_images <- mnist$train$x
train_labels <- mnist$train$y
# TODO improve
# TODO show adding a layer from another model
export_savedmodel(model_dense, "models/savedmodel")
view_savedmodel("models/savedmodel")
serve_savedmodel("models/savedmodel", daemonized=TRUE)
# grab a random index from the "batch" index (the first axis)
digit_index <- sample.int(length(train_images[1,,]), 1)
digit <- train_images[digit_index,,] # <- one slice of tensor please :)
digit_raw <- base64enc::base64encode(digit)
plot(as.raster(digit, max=255))
curl_cmd <- 'curl -X POST "http://127.0.0.1:8089/predict/predict/" \
-H "accept: application/json"                       \
-H "Content-Type: application/json"                 \
-d "{ \"instances\": [ { \"disp\": [ 160 ], \"cyl\": [ 4 ] } ]}"'
model_req_body <- paste('{"instances": [{"dense_3_input":', digit, '}]}')
model_req <- POST("http://127.0.0.1:8089/serving_default/predict/",
encode="json",
body=model_req_body)
model_req_body <- paste('{"instances": [{"dense_3_input":{"', digit_raw, '"}]}')
digit_raw <- base64enc::base64encode(digit)
model_req_body <- paste('{"instances": [{"dense_3_input":{"', digit_raw, '"}]}')
model_req <- POST("http://127.0.0.1:8089/serving_default/predict/",
encode="json",
body=model_req_body)
