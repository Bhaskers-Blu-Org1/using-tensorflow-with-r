knitr::opts_chunk$set(echo = TRUE)
library(keras)
library(here)
library(httr)
library(jsonlite)
library(tidyverse)
# devtools::install_github("rstudio/tfdeploy")
library(tfdeploy)
model_dense <- load_model_hdf5("models/mnist_dense_hdf5.h5")
load_model_weights_hdf5(model_dense, "models/mnist_dense_weights_hdf5.h5")
mnist <- dataset_mnist()
train_images <- mnist$train$x
train_labels <- mnist$train$y
# TODO improve
# TODO show adding a layer from another model
export_savedmodel(model_dense, "models/savedmodel")
view_savedmodel("models/savedmodel")
serve_savedmodel("models/savedmodel", daemonized=TRUE)
# grab a random index from the "batch" index (the first axis)
digit_index <- sample.int(length(train_images[1,,]), 1)
digit <- train_images[digit_index,,] # <- one slice of tensor please :)
digit_raw <- base64enc::base64encode(digit)
plot(as.raster(digit, max=255))
knitr::opts_chunk$set(echo = TRUE)
library(keras)
library(here)
library(httr)
library(jsonlite)
library(tidyverse)
# devtools::install_github("rstudio/tfdeploy")
library(tfdeploy)
model_dense <- load_model_hdf5("models/mnist_dense_hdf5.h5")
library(here)
library(keras)
library(tidyverse)
mnist <- dataset_mnist()
str(mnist$train)
train_images <- mnist$train$x
train_labels <- mnist$train$y
paste("How many dimensions does this tensor have?")
length(dim(train_images))
paste("What shape does this tensor have?")
dim(train_images)
paste("What datatype does this tensor have?")
typeof(train_images)
# grab a random index from the "batch" index (the first axis)
digit_index <- sample.int(length(train_images[1,,]), 1)
digit <- train_images[digit_index,,] # <- one slice of tensor please :)
plot(as.raster(digit, max=255))
train_images <- array_reshape(train_images, c(60000, 28 * 28))
train_images <- train_images / 255
head(train_images)
train_labels <- to_categorical(train_labels)
model <- keras_model_sequential() %>% # initialize our model using a pre-baked model type (thanks R keras!)
layer_dense(units = 512, activation = "relu", input_shape = c(28 * 28)) %>% # define our first layer transformation
layer_dense(units = 10, activation = "softmax") # define the final layer transformation
basic_relu <- function(my_2d_tensor) {
for (row in nrow(my_2d_tensor)) {
for (column in ncol(my_2d_tensor)) {
# the individual entry
my_2d_tensor[row, column] <- max(my_2d_tensor[row, column], 0)
}
}
}
model %>% compile(
optimizer = "rmsprop",
loss = "categorical_crossentropy",
metrics = c("accuracy")
)
test_images <- mnist$test$x
test_images <- array_reshape(test_images, c(10000, 28 * 28))
test_images <- test_images / 255
test_labels <- mnist$test$y
test_labels <- to_categorical(test_labels)
model
get_weights(model)
library(here)
library(keras)
library(tidyverse)
mnist <- dataset_mnist()
str(mnist$train)
train_images <- mnist$train$x
train_labels <- mnist$train$y
paste("How many dimensions does this tensor have?")
length(dim(train_images))
paste("What shape does this tensor have?")
dim(train_images)
paste("What datatype does this tensor have?")
typeof(train_images)
# grab a random index from the "batch" index (the first axis)
digit_index <- sample.int(length(train_images[1,,]), 1)
digit <- train_images[digit_index,,] # <- one slice of tensor please :)
plot(as.raster(digit, max=255))
train_images <- array_reshape(train_images, c(60000, 28 * 28))
train_images <- train_images / 255
head(train_images)
train_labels <- to_categorical(train_labels)
model <- keras_model_sequential() %>% # initialize our model using a pre-baked model type (thanks R keras!)
layer_dense(units = 512, activation = "relu", input_shape = c(28 * 28)) %>% # define our first layer transformation
layer_dense(units = 10, activation = "softmax") # define the final layer transformation
basic_relu <- function(my_2d_tensor) {
for (row in nrow(my_2d_tensor)) {
for (column in ncol(my_2d_tensor)) {
# the individual entry
my_2d_tensor[row, column] <- max(my_2d_tensor[row, column], 0)
}
}
}
model %>% compile(
optimizer = "rmsprop",
loss = "categorical_crossentropy",
metrics = c("accuracy")
)
test_images <- mnist$test$x
test_images <- array_reshape(test_images, c(10000, 28 * 28))
test_images <- test_images / 255
test_labels <- mnist$test$y
test_labels <- to_categorical(test_labels)
model
get_weights(model)
# replace the "" with the name of one of the layers printed in the previous step
get_layer(model, name="dense_1")
# replace the "" with the name of one of the layers printed in the previous step
get_layer(model, name="dense_31")
# replace the "" with the name of one of the layers printed in the previous step
get_layer(model, name="dense_3")
get_weights(model)
history <- model %>% fit(train_images, train_labels, epochs = 5, batch_size = 128)
library(here)
library(keras)
library(tidyverse)
mnist <- dataset_mnist()
str(mnist$train)
train_images <- mnist$train$x
train_labels <- mnist$train$y
paste("How many dimensions does this tensor have?")
length(dim(train_images))
paste("What shape does this tensor have?")
dim(train_images)
paste("What datatype does this tensor have?")
typeof(train_images)
# grab a random index from the "batch" index (the first axis)
digit_index <- sample.int(length(train_images[1,,]), 1)
digit <- train_images[digit_index,,] # <- one slice of tensor please :)
plot(as.raster(digit, max=255))
train_images <- array_reshape(train_images, c(60000, 28 * 28))
train_images <- train_images / 255
head(train_images)
train_labels <- to_categorical(train_labels)
model <- keras_model_sequential() %>% # initialize our model using a pre-baked model type (thanks R keras!)
layer_dense(units = 512, activation = "relu", input_shape = c(28 * 28)) %>% # define our first layer transformation
layer_dense(units = 10, activation = "softmax") # define the final layer transformation
basic_relu <- function(my_2d_tensor) {
for (row in nrow(my_2d_tensor)) {
for (column in ncol(my_2d_tensor)) {
# the individual entry
my_2d_tensor[row, column] <- max(my_2d_tensor[row, column], 0)
}
}
}
model %>% compile(
optimizer = "rmsprop",
loss = "categorical_crossentropy",
metrics = c("accuracy")
)
test_images <- mnist$test$x
test_images <- array_reshape(test_images, c(10000, 28 * 28))
test_images <- test_images / 255
test_labels <- mnist$test$y
test_labels <- to_categorical(test_labels)
model
get_weights(model)
model %>% compile(
optimizer = "rmsprop",
loss = "categorical_crossentropy",
metrics = c("accuracy")
)
model
get_weights(model)
library(keras)
mnist <- dataset_mnist()
str(mnist$train)
train_images <- mnist$train$x
train_labels <- mnist$train$y
paste("How many dimensions does this tensor have?")
length(dim(train_images))
paste("What shape does this tensor have?")
dim(train_images)
paste("What datatype does this tensor have?")
typeof(train_images)
# grab a random index from the "batch" index (the first axis)
digit_index <- sample.int(length(train_images[1,,]), 1)
digit <- train_images[digit_index,,] # <- one slice of tensor please :)
plot(as.raster(digit, max=255))
train_images <- array_reshape(train_images, c(60000, 28 * 28))
train_images <- train_images / 255
train_images
train_labels <- to_categorical(train_labels)
model <- keras_model_sequential() %>% # initialize our model using a pre-baked model type (thanks R keras!)
layer_dense(units = 512, activation = "relu", input_shape = c(28 * 28)) %>% # define our first layer transformation
layer_dense(units = 10, activation = "softmax") # define the final layer transformation
basic_relu <- function(my_2d_tensor) {
for (row in nrow(my_2d_tensor)) {
for (column in ncol(my_2d_tensor)) {
# the individual entry
my_2d_tensor[row, column] <- max(my_2d_tensor[row, column], 0)
}
}
}
model %>% compile(
optimizer = "rmsprop",
loss = "categorical_crossentropy",
metrics = c("accuracy")
)
test_images <- mnist$test$x
test_images <- array_reshape(test_images, c(10000, 28 * 28))
test_images <- test_images / 255
test_labels <- mnist$test$y
test_labels <- to_categorical(test_labels)
model
get_weights(model)
