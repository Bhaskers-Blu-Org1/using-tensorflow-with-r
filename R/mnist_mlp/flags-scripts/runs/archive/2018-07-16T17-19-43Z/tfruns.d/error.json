{
  "message": "object 'dense_units1' not found",
  "traceback": ["create_layer(keras$layers$Dense, object, list(units = as.integer(units), \n    activation = activation, use_bias = use_bias, kernel_initializer = kernel_initializer, \n    bias_initializer = bias_initializer, kernel_regularizer = kernel_regularizer, \n    bias_regularizer = bias_regularizer, activity_regularizer = activity_regularizer, \n    kernel_constraint = kernel_constraint, bias_constraint = bias_constraint, \n    input_shape = normalize_shape(input_shape), batch_input_shape = normalize_shape(batch_input_shape), \n    batch_size = as_nullable_integer(batch_size), dtype = dtype, \n    name = name, trainable = trainable, weights = weights))", "layer_dense(., units = dense_units1, activation = \"relu\", input_shape = c(784))", "function_list[[i]](value)", "freduce(value, `_function_list`)", "`_fseq`(`_lhs`)", "eval(quote(`_fseq`(`_lhs`)), env, env)", "eval(quote(`_fseq`(`_lhs`)), env, env)", "withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))", "model %>% layer_dense(units = dense_units1, activation = \"relu\", \n    input_shape = c(784)) %>% layer_dropout(rate = dropout1) %>% \n    layer_dense(units = dense_units2, activation = \"relu\") %>% \n    layer_dropout(rate = dropout2) %>% layer_dense(units = 10, \n    activation = \"softmax\")", "eval(ei, envir)", "eval(ei, envir)", "withVisible(eval(ei, envir))", "tfruns::tuning_run(\"mnist_mlp_FLAGS_TUNING.R\", flags = list(dense_unit1 = c(128, \n    256), dropout1 = c(0.2, 0.3), dense_unit2 = c(128, 256), \n    dropout2 = c(0.2, 0.3), epochs = c(20, 30)))"]
}
